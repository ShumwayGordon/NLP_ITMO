{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "Copy of HW1.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEEKQwtYDMUz"
      },
      "source": [
        "**SOFT DEADLINE:** `20.03.2022 23:59 msk` "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-IAo9sYDMU-"
      },
      "source": [
        "# [5 points] Part 1. Data cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SNyjuGjDMU_"
      },
      "source": [
        "The task is to clear the text data of the crawled web-pages from different sites. \n",
        "\n",
        "It is necessary to ensure that the distribution of the 100 most frequent words includes only meaningful words in english language (not particles, conjunctions, prepositions, numbers, tags, symbols)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TA4VkGl2DMVB"
      },
      "source": [
        "Determine the order of operations below and carry out the appropriate cleaning.\n",
        "\n",
        "1. Remove non-english words\n",
        "1. Remove html-tags (try to do it with regular expression, or play with beautifulsoap library)\n",
        "1. Apply lemmatization / stemming\n",
        "1. Remove stop-words\n",
        "1. Additional processing - At your own initiative, if this helps to obtain a better distribution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjLsB9pdDMVC"
      },
      "source": [
        "#### Hints"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mk6ZLncvDMVD"
      },
      "source": [
        "1. To do text processing you may use nltk and re libraries\n",
        "1. and / or any other libraries on your choise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCbTa1OiDMVE"
      },
      "source": [
        "#### Data reading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXbG649IDMVG"
      },
      "source": [
        "The dataset for this part can be downloaded here: `https://drive.google.com/file/d/1wLwo83J-ikCCZY2RAoYx8NghaSaQ-lBA/view?usp=sharing`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HN8UxSDkDMVH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGBTg9g4DMVJ"
      },
      "source": [
        "#### Data processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbpM3QUwDMVK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hklvhb6RDMVL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H59-VAGmDMVM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_h_c-1kBDMVN"
      },
      "source": [
        "#### Vizualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTu83iAWDMVN"
      },
      "source": [
        "As a visualisation, it is necessary to construct a frequency distribution of words (the 100 most common words), sorted by frequency. \n",
        "\n",
        "For visualization purposes we advice you to use plotly, but you are free to choose other libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHX6IXcrDMVO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZxcPfLTDMVO"
      },
      "source": [
        "#### Provide examples of processed text (some parts)\n",
        "\n",
        "Is everything all right with the result of cleaning these examples? What kind of information was lost?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2BSMfa9lDMVP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xn6XK7UDMVP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IlSeYg4sDMVQ"
      },
      "source": [
        "# [10 points] Part 2. Duplicates detection. LSH"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZuMBHSiDDMVR"
      },
      "source": [
        "#### Libraries you can use\n",
        "\n",
        "1. LSH - https://github.com/ekzhu/datasketch\n",
        "1. LSH - https://github.com/mattilyra/LSH\n",
        "1. Any other library on your choise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uff5aBpgDMVR"
      },
      "source": [
        "1. Detect duplicated text (duplicates do not imply a complete word-to-word match, but texts that may contain a paraphrase, rearrangement of words, sentences)\n",
        "1. Make a plot dependency of duplicates on shingle size (with fixed minhash length) \n",
        "1. Make a plot dependency of duplicates on minhash length (with fixed shingle size)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOFZ7AvyDMVS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPonm9U3DMVS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njdTa1daDMVS"
      },
      "source": [
        "# [Optional 10 points] Part 3. Topic model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BwJNkn2DMVT"
      },
      "source": [
        "In this part you will learn how to do topic modeling with common tools and assess the resulting quality of the models. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0WcLMhHDMVT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkNFzxqPDMVU"
      },
      "source": [
        "The provided data contain chunked stories by Edgar Allan Poe (EAP), Mary Shelley (MWS), and HP Lovecraft (HPL).\n",
        "\n",
        "The dataset can be downloaded here: `https://drive.google.com/file/d/14tAjAzHr6UmFVFV7ABTyNHBh-dWHAaLH/view?usp=sharing`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BNIa1miDMVU"
      },
      "source": [
        "#### Preprocess dataset with the functions from the Part 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9B8PZNRyDMVV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKU2D29CDMVV"
      },
      "source": [
        "#### Quality estimation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6xfd3vpDMVW"
      },
      "source": [
        "Implement the following three quality fuctions: `coherence` (or `tf-idf coherence`), `normalized PMI`, `based on the distributed word representation`(you can use pretrained w2v vectors or some other model). You are free to use any libraries (for instance gensim) and components."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEtv-gWcDMVW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kp1jTJmPDMVX"
      },
      "source": [
        "### Topic modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgbVtgphDMVX"
      },
      "source": [
        "Read and preprocess the dataset, divide it into train and test parts `sklearn.model_selection.train_test_split`. Test part will be used in classification part. For simplicity we do not perform cross-validation here, but you should remember about it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Uh0J_C8DMVY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNepVz1wDMVY"
      },
      "source": [
        "Plot the histogram of resulting tokens counts in the processed datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_7JVEfbDMVZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulOjNL9vDMVZ"
      },
      "source": [
        "Plot the histogram of resulting tokens counts in the processed datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAse0LBvDMVa"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4N86AaGNDMVa"
      },
      "source": [
        "#### NMF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPelcAFQDMVa"
      },
      "source": [
        "Implement topic modeling with NMF (you can use `sklearn.decomposition.NMF`) and print out resulting topics. Try to change hyperparameters to better fit the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCPzCMMMDMVa"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuaKAGIWDMVb"
      },
      "source": [
        "#### LDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAK_MfQxDMVb"
      },
      "source": [
        "Implement topic modeling with LDA (you can use gensim implementation) and print out resulting topics. Try to change hyperparameters to better fit the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdzopuCJDMVb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSF6fq0NDMVc"
      },
      "source": [
        "### Additive regularization of topic models "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVcA4itQDMVc"
      },
      "source": [
        "Implement topic modeling with ARTM. You may use bigartm library (simple installation for linux: pip install bigartm) or TopicNet framework (`https://github.com/machine-intelligence-laboratory/TopicNet`)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03xhr0PmDMVc"
      },
      "source": [
        "Create artm topic model fit it to the data. Try to change hyperparameters (number of specific and background topics) to better fit the dataset. Play with smoothing and sparsing coefficients (use grid), try to add decorrelator. Print out resulting topics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgv1qIUZDMVc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mteOYV8TDMVd"
      },
      "source": [
        "Write a function to convert new documents to topics probabilities vectors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGkD9YaxDMVd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPrNOdMLDMVd"
      },
      "source": [
        "Calculate the quality scores for each model. Make a barplot to compare the quality."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQadAQCXDMVd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}